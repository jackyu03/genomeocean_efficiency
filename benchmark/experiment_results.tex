\documentclass[12pt]{article}
\usepackage{openwork}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{hyperref}

\title{GenomeOcean: Efficiency and Representational Fidelity of Genomic Foundation Models on H100 Accelerators}
\author[1]{First Author}
\author[2]{Second Author}
\affil[1]{Department of Genomics, University Name}
\affil[2]{Department of Computer Science, University Name}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
As genomic foundation models scale towards billions of parameters, the computational cost of inference becomes a critical bottleneck for high-throughput biological discovery. This study systematically evaluates the performance trade-offs of the GenomeOcean architecture (100M and 4B parameters) on next-generation NVIDIA H100 hardware. By leveraging native hardware support for FP8 precision alongside standard BFloat16, we quantify the impact of quantization on three dimensions: computational efficiency (throughput and power), biological syntax generation (perplexity), and downstream utility (species binning). Our results demonstrate that while FP8 quantization offers substantial efficiency gains, its impact on representational fidelity varies significantly with model scale, necessitating a careful selection of precision for deployment in resource-constrained environments.
\end{abstract}

\section{Introduction}
Genomic Language Models (gLMs) have demonstrated remarkable capabilities in learning the syntax and semantics of biological sequences. However, deploying these models at scale poses significant challenges due to their memory footprint and computational latency. The recent advent of hardware-accelerated low-precision inference, notably FP8 support on NVIDIA H100 GPUs, presents an opportunity to mitigate these costs. This work provides a rigorous benchmark of the GenomeOcean family of models, assessing whether reduced-precision inference can maintain the fidelity required for sensitive biological tasks such as metagenomic binning and sequence generation.

\section{Methods and Experimental Setup}

\subsection{Hardware Configuration}
All experiments were conducted on a single NVIDIA H100 SXM5 GPU (80GB VRAM). We utilized the device's native Transformer Engine to support mixed-precision inference. Two quantization modes were evaluated:
\begin{enumerate}
    \item \textbf{BFloat16 (BF16)}: The standard training precision, serving as the high-fidelity baseline.
    \item \textbf{FP8 (E4M3)}: A prominent 8-bit floating-point format designed for deep learning inference, offering a theoretical 2x throughput improvement over BF16.
\end{enumerate}

\subsection{Model Architectures}
We evaluated two distinct scales of the GenomeOcean architecture to understand how parameter count influences quantization resilience:
\begin{itemize}
    \item \textbf{GenomeOcean-100M}: A lightweight model optimized for rapid inference, with a maximum context window of 1,024 tokens.
    \item \textbf{GenomeOcean-4B}: A large-scale foundation model designed for capturing long-range genomic dependencies, supporting a context window of 10,240 tokens.
\end{itemize}

\subsection{Benchmarking Protocols}
We established two rigorous benchmarks to evaluate efficiency and quality independently.

\subsubsection{Protocol A: Efficiency and Embedding Utility}
This protocol measures the raw computational performance and the semantic quality of the model's internal representations.
\begin{itemize}
    \item \textbf{Dataset}: We curated a dataset (\textit{bac120\_50\_seq\_50k\_nonoverlap}) consisting of 20 randomly selected genomes from the GTDB Bacteria database. To ensure robust statistics, we processed approximately $1 \times 10^6$ tokens per genome.
    \item \textbf{Fragmentation Strategy}: To accommodate the disparate context windows while maintaining comparable data coverage, we adopted model-specific fragmentation strategies:
    \begin{itemize}
        \item \textit{GenomeOcean-100M}: Large genomic fragments were subdivided into non-overlapping 5,000bp chunks. The model processed these as 1,024-token sequences. With $N=1000$ fragments, this approach yielded an effective data utilization of $\sim96\%$.
        \item \textit{GenomeOcean-4B}: The model processed unmodified fragments using its full 10,240-token context window. With $N=100$ fragments, this resulted in nearly $100\%$ data utilization.
    \end{itemize}
    \item \textbf{Downstream Task (Species Binning)}: To quantify semantic utility, we extracted the last-layer embeddings and applied DBSCAN clustering ($\epsilon=0.5$, min\_samples=5). Performance was evaluated using the Adjusted Rand Index (ARI), which measures the agreement between the model-induced clusters and ground-truth taxonomic labels.
\end{itemize}

\subsubsection{Protocol B: Causal Generation and Compliance}
This protocol evaluates the model's ability to generate biologically valid sequences, a proxy for its "understanding" of genomic grammar.
\begin{itemize}
    \item \textbf{Dataset}: A larger evaluation set of 100 genomes (\textit{bac120\_100\_seq\_550kbp\_generation\_use}) was used to ensure statistical significance.
    \item \textbf{Evaluation Metrics}: We computed Perplexity (PPL) and Negative Log-Likelihood (NLL) to evaluate scoring fidelity. We also conducted raw generation throughput stress tests (tokens/second) to evaluate the efficiency of the decoding phase under maximum parallel load (batch size = 128).
    \item \textbf{Energy Measurement}: Total power draw (Watts) was measured during generation using active NVIDIA Management Library (NVML) polling via a custom \texttt{EnergyMeter} to establish token-per-watt efficiency.
\end{itemize}

\section{Results}

\subsection{Computational Efficiency}
Table \ref{tab:efficiency} presents the throughput (tokens/second) and power consumption (Watts).

\begin{table}[h]
    \centering
    \caption{\textbf{Computational Efficiency on H100.} Comparison of throughput, power consumption, and memory usage across precision modes.}
    \label{tab:efficiency}
    \small
    \begin{tabular}{llcccc}
        \toprule
        \textbf{Model} & \textbf{Precision} & \textbf{Throughput} & \textbf{Power} & \textbf{Memory} & \textbf{Efficiency} \\
        & & (tok/s) & (W) & (GB) & (tok/W) \\
        \midrule
        \textbf{100M} & BF16 & 620,142 & 621.6 & 21.15 & 997.6 \\
                      & FP8  & 459,312 & 539.0 & 21.03 & 852.1 \\
        \midrule
        \textbf{4B}   & BF16 & 21,401 & 698.5 & 72.14 & 30.6 \\
                      & FP8  & 22,453 & 691.8 & 67.91 & 32.5 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Generative Fidelity and Throughput}
Table \ref{tab:generation} summarizes the GenomeOcean-4B model's ability to model the genomic probability distribution alongside its raw decoding throughput when processing massive concurrent batches.

\begin{table}[h]
    \centering
    \caption{\textbf{Generative Performance and Decoding Throughput.} Metric evaluations (Perplexity, NLL) and generation efficiency (throughput, power, VRAM) on the unified generation benchmark (Batch Size = 128).}
    \label{tab:generation}
    \small
    \begin{tabular}{llccccc}
        \toprule
        \textbf{Model} & \textbf{Precision} & \textbf{Perplexity} & \textbf{NLL} & \textbf{Throughput (\(\frac{\text{tok}}{\text{s}}\))} & \textbf{Power (W)} & \textbf{Peak VRAM (GB)} \\
        \midrule
        \textbf{4B}   & BF16 & 84.85 \(\pm\) 22.08 & 4.42 \(\pm\) 0.26 & 3214.82 & 632.02 & 76.07 \\
                      & FP8  & 84.86 \(\pm\) 22.08 & 4.42 \(\pm\) 0.26 & 2067.20 & 532.27 & 76.30 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Downstream Utility}
Table \ref{tab:binning} illustrates the quality of embeddings for taxonomic classification.

\begin{table}[h]
    \centering
    \caption{\textbf{Species Binning Utility.} Clustering performance measured by Adjusted Rand Index (ARI).}
    \label{tab:binning}
    \small
    \begin{tabular}{llc}
        \toprule
        \textbf{Model} & \textbf{Precision} & \textbf{ARI} \\
        \midrule
        \textbf{100M} & BF16 & 0.330 \\
                      & FP8  & 0.334 \\
        \midrule
        \textbf{4B}   & BF16 & 0.490 \\
                      & FP8  & 0.491 \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Discussion}
The experimental results present a nuanced narrative regarding the efficacy of low-precision inference on the NVIDIA H100 platform. While the shift from BFloat16 to FP8 is often touted as a universal accelerant, our data indicates that the realized gains are strongly contingent on model scale and workload characteristics (context length).

\subsection{The "Small Model Trap": Kernel Overhead vs. Arithmetic Density}
The most striking, and indeed confounding, result is the \textbf{26\% throughput regression} observed in the GenomeOcean-100M model under FP8 quantization. Theoretically, the H100's fourth-generation Tensor Cores offer a $2\times$ peak FLOPs advantage for FP8 over BF16. However, this theoretical ceiling is only approachable under high arithmetic intensity (compute-bound) regimes with sufficiently large matrix dimensions to saturate the GPU's massive 80GB/s SM throughput.

We hypothesize that for the 100M parameter model, the execution is latency-bound rather than throughput-bound. The overhead of quantization-specific operations---dynamic scaling of activation tensors, casting, and the launch latency of specialized FP8 kernels---likely eclipses the reduction in execution time for the matrix multiplications (GEMMs) themselves. Since the dot-products in a 100M model are relatively small, the "setup cost" of FP8 becomes a dominant factor, leading to the observed deceleration. This effectively creates a "Small Model Trap" where next-generation scaling techniques yield negative returns.

\subsection{Memory Analysis: The Dominance of KV Cache}
For the GenomeOcean-4B model, we clearly observe the "Weight-Quantization Ceiling."
\begin{enumerate}
    \item \textbf{Theoretical Expectations}: The 4B model contains approximately $4 \times 10^9$ parameters. Converting these weights from BF16 (2 bytes) to FP8 (1 byte) should yield a static memory saving of exactly 4 GB ($8 \text{GB} \to 4 \text{GB}$).
    \item \textbf{Empirical Observation}: Our results show a VRAM reduction from 72.14 GB to 67.91 GB, a difference of \textbf{4.23 GB}.
\end{enumerate}
This near-perfect alignment confirms that the memory savings are strictly limited to weight tensors. The remaining $\sim$68 GB of utilization is dominated by the \textbf{Key-Value (KV) Cache}, necessitated by the long-context benchmarking protocol (16 batch size $\times$ 10,240 tokens).
$$ \text{Memory}_{KV} \approx 2 \cdot L \cdot H \cdot N_{seq} \cdot N_{batch} \cdot \text{Size}_{dtype} $$
Given the overwhelming dominance of the KV cache in this long-context regime (accounting for $>90\%$ of VRAM usage), the efficiency gains from weight quantization are mathematically diluted using standard attention implementations. The 5.9\% memory reduction, while seemingly small, represents the maximum theoretical limit achievable without compressing the KV cache itself (e.g., via PagedAttention or KV-quantization).

\subsection{Conclusion: Baffling Stagnation in Throughput}
Perhaps the most "baffling" outcome in the embedding extraction benchmark (Table \ref{tab:efficiency}) is the modest \textbf{4.9\% throughput gain} for the 4B model. With memory bandwidth pressure reduced by fetching 8-bit weights and compute capacity doubled, a larger speedup was anticipated. We hypothesize that the bottleneck has shifted from memory bandwidth to the \textbf{Attention Mechanism} itself. Since native FP8 implementation quantizes Linear layers but retains high-precision (BF16 or FP32) accumulation for the Softmax attention scores to preserve stability, the $O(N^2)$ attention operation remains a high-precision choke point.

\subsection{The FP8 KV-Cache Decoding Regression and Software Limitations}
When transitioning from the embedding extraction benchmark to the pure generative decoding benchmark, an unexpected performance regression is observed. As shown in Table \ref{tab:generation}, quantizing the Key-Value (KV) cache to FP8 successfully preserves the biological representational fidelity of the model, maintaining a Negative Log-Likelihood of 4.42, while concurrently halving the theoretical memory footprint required for the context window. However, this quantization simultaneously results in a 35.6\% reduction in overall decoding throughput, dropping from 3214.82 tokens per second to 2067.20 tokens per second when compared to the native BF16 execution baseline. This performance degradation does not originate from a limitation in the NVIDIA H100 hardware architecture. Instead, it is a direct result of software compilation constraints and dependency management challenges inherent to modern large language model serving engines when deployed on complex High-Performance Computing clusters.

\subsubsection{The Compilation Fallback Penalty}
To achieve the theoretical hardware acceleration associated with FP8 precision during the attention decoding phase, the system requires highly optimized C++ CUDA kernels that are specifically designed to execute fused dequantization and matrix multiplication natively on the Tensor Cores. Leading attention libraries, such as FlashInfer, provide this specialized capability. Deploying these advanced CUDA extensions requires precise synchronization between the host compilers, including exact version matching across the GCC, NVCC, and Ninja binary toolchains. In shared High-Performance Computing environments, managing these rigid dependencies often leads to conflicts and Just-In-Time compilation failures. When the vLLM serving engine detects the absence of a successfully compiled FlashInfer binary, the system automatically defaults to less optimized attention backends, such as Python-wrapped Triton kernels or the standard XFormers library. In contrast, native BF16 execution on the H100 architecture is universally supported by the ubiquitous and pre-compiled FlashAttention-3 library, which is highly optimized to achieve near-peak theoretical GPU utilization. Consequently, the benchmark compares a fully optimized BF16 computational pathway against an unoptimized software fallback for FP8, leading to the observed disparity in throughput. 

\subsubsection{The On-the-Fly Dequantization Bottleneck}
The reduction in throughput is further compounded by the specific mechanisms through which these fallback kernels process sub-byte precision data. Because the default Triton and XFormers fallback kernels are primarily architected for 16-bit floating-point mathematics, they lack the capability to perform the core attention matrix operations directly on an 8-bit KV cache. During every autoregressive decoding step, the GPU is forced to execute a computationally expensive sequence of data type conversions. The hardware must first fetch the FP8 KV tensors from High Bandwidth Memory and dynamically cast, or dequantize, these tensors into BF16 formats within the Streaming Multiprocessor registers. The Tensor Cores then execute the BF16 matrix multiplication, after which the resultant token state must be quantized back into FP8 and written back to the High Bandwidth Memory. This continuous, on-the-fly casting overhead introduces significant micro-latency into the decoding loop. The computational cost of these repeated conversions entirely offsets the bandwidth savings that would otherwise be gained by fetching the smaller 8-bit payloads from memory.

Additionally, the empirical data shows that VRAM consumption peaks identically at approximately 76 GB for both the BF16 and FP8 precision formats. This parity highlights a specific architectural behavior within the memory management system of the vLLM engine. Upon initialization, the system is configured to preemptively reserve 90\% of the global High Bandwidth Memory block allocation. As a result, quantizing the KV cache to FP8 does not actively release physical memory back to the operating system. Rather, the quantization functionally doubles the internal block capacity of the pre-allocated memory pool. This structural density allows the serving engine to sustain twice the concurrent batch size or sequence length before it encounters an Out-Of-Memory fault. These findings demonstrate that while FP8 serves as a highly effective memory compression technique for evaluating expansive genomic sequences, unlocking its theoretical speed advantages requires a fully synchronized compilation environment capable of deploying custom fused kernels. In the absence of such an environment, practitioners must accept a consequential reduction in generation speed in exchange for doubled contextual capacity.

\printbibliography

\end{document}