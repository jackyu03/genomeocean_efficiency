\documentclass[12pt]{article}
\usepackage{openwork}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{hyperref}

\title{GenomeOcean: Efficiency and Representational Fidelity of Genomic Foundation Models on H100 Accelerators}
\author[1]{First Author}
\author[2]{Second Author}
\affil[1]{Department of Genomics, University Name}
\affil[2]{Department of Computer Science, University Name}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
As genomic foundation models scale towards billions of parameters, the computational cost of inference becomes a critical bottleneck for high-throughput biological discovery. This study systematically evaluates the performance trade-offs of the GenomeOcean architecture (100M and 4B parameters) on next-generation NVIDIA H100 hardware. By leveraging native hardware support for FP8 precision alongside standard BFloat16, we quantify the impact of quantization on three dimensions: computational efficiency (throughput and power), biological syntax generation (perplexity), and downstream utility (species binning). Our results demonstrate that while FP8 quantization offers substantial efficiency gains, its impact on representational fidelity varies significantly with model scale, necessitating a careful selection of precision for deployment in resource-constrained environments.
\end{abstract}

\section{Introduction}
Genomic Language Models (gLMs) have demonstrated remarkable capabilities in learning the syntax and semantics of biological sequences. However, deploying these models at scale poses significant challenges due to their memory footprint and computational latency. The recent advent of hardware-accelerated low-precision inference, notably FP8 support on NVIDIA H100 GPUs, presents an opportunity to mitigate these costs. This work provides a rigorous benchmark of the GenomeOcean family of models, assessing whether reduced-precision inference can maintain the fidelity required for sensitive biological tasks such as metagenomic binning and sequence generation.

\section{Methods and Experimental Setup}

\subsection{Hardware Configuration}
All experiments were conducted on a single NVIDIA H100 SXM5 GPU equipped with 80 gigabytes of High Bandwidth Memory. We utilized the native Transformer Engine of the device to support mixed-precision inference. During the evaluations, two distinct quantization modes were analyzed. The first mode is BFloat16, which represents the standard training precision and serves as the high-fidelity baseline for all comparisons. The second mode is FP8, specifically the E4M3 variant, which is a prominent 8-bit floating-point format designed explicitly for deep learning inference workloads. This 8-bit format offers a theoretical twofold throughput improvement over the standard BFloat16 precision by effectively doubling the computational density and halving the memory bandwidth requirements during matrix operations.

\subsection{Model Architectures}
We evaluated two distinct scales of the GenomeOcean architecture to systematically understand how total parameter count influences the resilience of the model to quantization techniques. The first architecture is GenomeOcean-100M, which is a lightweight model specifically optimized for rapid, low-latency inference. This smaller model is configured with a maximum context window of 1024 tokens. The second architecture is GenomeOcean-4B, representing a large-scale genomic foundation model. This robust architecture is designed specifically for capturing complex, long-range genomic dependencies and supports an expansive context window of 10240 tokens, allowing for deep sequential biological analysis.

\subsection{Benchmarking Protocols}
We established two rigorous benchmarks to evaluate efficiency and quality independently.

\subsubsection{Protocol A: Efficiency and Embedding Utility}
This protocol measures both the raw computational performance characteristics and the semantic quality inherent to the internal representations generated by the model. For these experiments, we curated a specific evaluation dataset consisting of twenty randomly selected genomes sampled from the GTDB Bacteria database. To guarantee statistical robustness across all trials, the evaluation pipeline processed approximately one million tokens per genome. To seamlessly accommodate the highly disparate context windows of the two architectural scales while strictly maintaining comparable data coverage volume, we adopted distinct, model-specific fragmentation strategies. For the GenomeOcean-100M model, expansive genomic fragments were systematically subdivided into non-overlapping chunks of 5000 base pairs. The model subsequently processed these segments as 1024-token sequences. Operating with a sample size of one thousand individual fragments, this specific approach yielded an effective data utilization rate of approximately 96 percent. Conversely, the GenomeOcean-4B model was configured to process completely unmodified fragments by utilizing its full 10240-token context window. Operating with one hundred full-length fragments, this approach achieved nearly comprehensive data utilization. To accurately quantify the semantic utility derived from these processes, we extracted the corresponding last-layer hidden embeddings from the models. We then applied the DBSCAN clustering algorithm to these embeddings, utilizing an epsilon value of 0.5 and a minimum sample threshold of five. The overall downstream performance of this pipeline was rigorously evaluated using the Adjusted Rand Index, determining the exact level of mathematical agreement between the unsupervised model-induced clusters and the actual biological ground-truth taxonomic labels.

\subsubsection{Protocol B: Causal Generation and Compliance}
This secondary protocol systematically evaluates the capacity of the model to generate biologically valid sequences, serving as a direct quantitative proxy for its generalized understanding of underlying genomic grammar. For this phase, a substantially larger evaluation set comprised of one hundred full genomes was utilized to assure absolute statistical significance across all derived metrics. To evaluate the precise scoring fidelity of the architectures, we computed both standard Perplexity and Negative Log-Likelihood utilizing a sliding window context approach. Furthermore, we conducted automated raw generation throughput stress tests, measuring continuous tokens per second, to evaluate the fundamental hardware efficiency of the autoregressive decoding phase while operating under a maximum parallel load utilizing a batch size of 128. Concurrently, the total electrical power draw in Watts was actively measured throughout the entire generation cycle. This metric was recorded utilizing continuous polling through the NVIDIA Management Library via a custom energy measurement apparatus, allowing us to accurately establish the exact computational token-per-watt efficiency profile of the hardware.

\section{Results}

\subsection{Computational Efficiency}
Table \ref{tab:efficiency} presents the throughput (tokens/second) and power consumption (Watts).

\begin{table}[h]
    \centering
    \caption{\textbf{Computational Efficiency on H100.} Comparison of throughput, power consumption, and memory usage across precision modes.}
    \label{tab:efficiency}
    \small
    \begin{tabular}{llcccc}
        \toprule
        \textbf{Model} & \textbf{Precision} & \textbf{Throughput} & \textbf{Power} & \textbf{Memory} & \textbf{Efficiency} \\
        & & (tok/s) & (W) & (GB) & (tok/W) \\
        \midrule
        \textbf{100M} & BF16 & 620,142 & 621.6 & 21.15 & 997.6 \\
                      & FP8  & 459,312 & 539.0 & 21.03 & 852.1 \\
        \midrule
        \textbf{4B}   & BF16 & 21,401 & 698.5 & 72.14 & 30.6 \\
                      & FP8  & 22,453 & 691.8 & 67.91 & 32.5 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Generative Fidelity and Throughput}
Table \ref{tab:generation} summarizes the GenomeOcean-4B model's ability to model the genomic probability distribution alongside its raw decoding throughput when processing massive concurrent batches.

\begin{table}[h]
    \centering
    \caption{\textbf{Generative Performance and Decoding Throughput.} Metric evaluations (Perplexity, NLL) and generation efficiency (throughput, power, VRAM) on the unified generation benchmark (Batch Size = 128).}
    \label{tab:generation}
    \small
    \begin{tabular}{llccccc}
        \toprule
        \textbf{Model} & \textbf{Precision} & \textbf{Perplexity} & \textbf{NLL} & \textbf{Throughput (\(\frac{\text{tok}}{\text{s}}\))} & \textbf{Power (W)} & \textbf{Peak VRAM (GB)} \\
        \midrule
        \textbf{4B}   & BF16 & 84.85 \(\pm\) 22.08 & 4.42 \(\pm\) 0.26 & 3214.82 & 632.02 & 76.07 \\
                      & FP8  & 84.86 \(\pm\) 22.08 & 4.42 \(\pm\) 0.26 & 2067.20 & 532.27 & 76.30 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Downstream Utility}
Table \ref{tab:binning} illustrates the quality of embeddings for taxonomic classification.

\begin{table}[h]
    \centering
    \caption{\textbf{Species Binning Utility.} Clustering performance measured by Adjusted Rand Index (ARI).}
    \label{tab:binning}
    \small
    \begin{tabular}{llc}
        \toprule
        \textbf{Model} & \textbf{Precision} & \textbf{ARI} \\
        \midrule
        \textbf{100M} & BF16 & 0.330 \\
                      & FP8  & 0.334 \\
        \midrule
        \textbf{4B}   & BF16 & 0.490 \\
                      & FP8  & 0.491 \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Discussion}
The experimental results present a nuanced narrative regarding the efficacy of low-precision inference on the NVIDIA H100 platform. While the shift from BFloat16 to FP8 is often touted as a universal accelerant, our data indicates that the realized gains are strongly contingent on model scale and workload characteristics (context length).

\subsection{The "Small Model Trap": Kernel Overhead vs. Arithmetic Density}
The most striking, and indeed confounding, result is the \textbf{26\% throughput regression} observed in the GenomeOcean-100M model under FP8 quantization. Theoretically, the H100's fourth-generation Tensor Cores offer a $2\times$ peak FLOPs advantage for FP8 over BF16. However, this theoretical ceiling is only approachable under high arithmetic intensity (compute-bound) regimes with sufficiently large matrix dimensions to saturate the GPU's massive 80GB/s SM throughput.

We hypothesize that for the 100M parameter model, the execution is latency-bound rather than throughput-bound. The overhead of quantization-specific operations---dynamic scaling of activation tensors, casting, and the launch latency of specialized FP8 kernels---likely eclipses the reduction in execution time for the matrix multiplications (GEMMs) themselves. Since the dot-products in a 100M model are relatively small, the "setup cost" of FP8 becomes a dominant factor, leading to the observed deceleration. This effectively creates a "Small Model Trap" where next-generation scaling techniques yield negative returns.

\subsection{Memory Analysis: The Dominance of KV Cache}
For the GenomeOcean-4B architecture, the empirical data clearly illustrates a phenomenon known as the weight quantization ceiling. Theoretical models dictate that the 4B architecture contains approximately four billion parameters. Converting the entire parameter set from a 2-byte BF16 format to a 1-byte FP8 format should theoretically yield a strict, static memory saving of exactly four gigabytes. The empirical observations gathered during our profiling perfectly align with this mathematical expectation. Our recorded telemetry demonstrates a total Video RAM reduction from 72.14 gigabytes down to a baseline of 67.91 gigabytes, yielding an absolute net difference of 4.23 gigabytes. This precise structural alignment confirms conclusively that the physical memory savings observed in this paradigm are strictly isolated to the localized weight tensors themselves.

The remaining block of approximately 68 gigabytes of GPU utilization is entirely dominated by the Key-Value cache, which is mathematically necessitated by the long-context benchmarking protocol utilizing a batch size of 16 operating simultaneously on a 10240-token grid.
$$ \text{Memory}_{KV} \approx 2 \cdot L \cdot H \cdot N_{seq} \cdot N_{batch} \cdot \text{Size}_{dtype} $$
Given the overwhelming dominance of the Key-Value cache in this long-context regime, which accounts for greater than 90 percent of the total VRAM usage, the efficiency gains from weight quantization are mathematically diluted when utilizing standard attention implementations. The 5.9 percent total memory reduction, while seemingly small, represents the absolute maximum theoretical limit achievable without actively compressing the sequence Key-Value cache itself via secondary operations.

\subsection{Conclusion: Baffling Stagnation in Throughput}
Perhaps the most "baffling" outcome in the embedding extraction benchmark (Table \ref{tab:efficiency}) is the modest \textbf{4.9\% throughput gain} for the 4B model. With memory bandwidth pressure reduced by fetching 8-bit weights and compute capacity doubled, a larger speedup was anticipated. We hypothesize that the bottleneck has shifted from memory bandwidth to the \textbf{Attention Mechanism} itself. Since native FP8 implementation quantizes Linear layers but retains high-precision (BF16 or FP32) accumulation for the Softmax attention scores to preserve stability, the $O(N^2)$ attention operation remains a high-precision choke point.

\subsection{The FP8 KV-Cache Decoding Regression and Software Limitations}
When transitioning from the embedding extraction benchmark to the pure generative decoding benchmark, an unexpected performance regression is observed. As shown in Table \ref{tab:generation}, quantizing the Key-Value (KV) cache to FP8 successfully preserves the biological representational fidelity of the model, maintaining a Negative Log-Likelihood of 4.42, while concurrently halving the theoretical memory footprint required for the context window. However, this quantization simultaneously results in a 35.6\% reduction in overall decoding throughput, dropping from 3214.82 tokens per second to 2067.20 tokens per second when compared to the native BF16 execution baseline. This performance degradation does not originate from a limitation in the NVIDIA H100 hardware architecture. Instead, it is a direct result of software compilation constraints and dependency management challenges inherent to modern large language model serving engines when deployed on complex High-Performance Computing clusters.

\subsubsection{The Compilation Fallback Penalty}
To achieve the theoretical hardware acceleration associated with FP8 precision during the attention decoding phase, the system requires highly optimized C++ CUDA kernels that are specifically designed to execute fused dequantization and matrix multiplication natively on the Tensor Cores. Leading attention libraries, such as FlashInfer, provide this specialized capability. Deploying these advanced CUDA extensions requires precise synchronization between the host compilers, including exact version matching across the GCC, NVCC, and Ninja binary toolchains. In shared High-Performance Computing environments, managing these rigid dependencies often leads to conflicts and Just-In-Time compilation failures. When the vLLM serving engine detects the absence of a successfully compiled FlashInfer binary, the system automatically defaults to less optimized attention backends, such as Python-wrapped Triton kernels or the standard XFormers library. In contrast, native BF16 execution on the H100 architecture is universally supported by the ubiquitous and pre-compiled FlashAttention-3 library, which is highly optimized to achieve near-peak theoretical GPU utilization. Consequently, the benchmark compares a fully optimized BF16 computational pathway against an unoptimized software fallback for FP8, leading to the observed disparity in throughput. 

\subsubsection{The On-the-Fly Dequantization Bottleneck}
The reduction in throughput is further compounded by the specific mechanisms through which these fallback kernels process sub-byte precision data. Because the default Triton and XFormers fallback kernels are primarily architected for 16-bit floating-point mathematics, they lack the capability to perform the core attention matrix operations directly on an 8-bit KV cache. During every autoregressive decoding step, the GPU is forced to execute a computationally expensive sequence of data type conversions. The hardware must first fetch the FP8 KV tensors from High Bandwidth Memory and dynamically cast, or dequantize, these tensors into BF16 formats within the Streaming Multiprocessor registers. The Tensor Cores then execute the BF16 matrix multiplication, after which the resultant token state must be quantized back into FP8 and written back to the High Bandwidth Memory. This continuous, on-the-fly casting overhead introduces significant micro-latency into the decoding loop. The computational cost of these repeated conversions entirely offsets the bandwidth savings that would otherwise be gained by fetching the smaller 8-bit payloads from memory.

Additionally, the empirical data shows that VRAM consumption peaks identically at approximately 76 GB for both the BF16 and FP8 precision formats. This parity highlights a specific architectural behavior within the memory management system of the vLLM engine. Upon initialization, the system is configured to preemptively reserve 90\% of the global High Bandwidth Memory block allocation. As a result, quantizing the KV cache to FP8 does not actively release physical memory back to the operating system. Rather, the quantization functionally doubles the internal block capacity of the pre-allocated memory pool. This structural density allows the serving engine to sustain twice the concurrent batch size or sequence length before it encounters an Out-Of-Memory fault. These findings demonstrate that while FP8 serves as a highly effective memory compression technique for evaluating expansive genomic sequences, unlocking its theoretical speed advantages requires a fully synchronized compilation environment capable of deploying custom fused kernels. In the absence of such an environment, practitioners must accept a consequential reduction in generation speed in exchange for doubled contextual capacity.

\printbibliography

\end{document}