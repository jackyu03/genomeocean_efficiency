{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Benchmark Results Viewer\n",
    "\n",
    "This notebook loads and displays results from a quantization benchmark analysis folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select Analysis Folder\n",
    "\n",
    "Change this to your analysis folder path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to your analysis folder\n",
    "analysis_folder = \"./results/analysis_20251116_114713\"  # Example timestamp\n",
    "\n",
    "# Or list all available analysis folders\n",
    "results_dir = Path(\"./results\")\n",
    "analysis_folders = sorted([f for f in results_dir.glob(\"analysis_*\") if f.is_dir()])\n",
    "print(\"Available analysis folders:\")\n",
    "for i, folder in enumerate(analysis_folders):\n",
    "    print(f\"{i}: {folder.name}\")\n",
    "\n",
    "# Use the most recent one\n",
    "if analysis_folders:\n",
    "    analysis_folder = str(analysis_folders[-1])\n",
    "    print(f\"\\nUsing: {analysis_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Summary\n",
    "\n",
    "Main performance metrics by quantization mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load performance summary\n",
    "quant_summary = pd.read_csv(f\"{analysis_folder}/quant_summary.csv\")\n",
    "print(\"\\n=== Performance Summary ===\")\n",
    "display(quant_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load memory analysis\n",
    "memory_analysis = pd.read_csv(f\"{analysis_folder}/memory_analysis.csv\")\n",
    "print(\"\\n=== Memory Analysis ===\")\n",
    "display(memory_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Throughput Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load throughput comparison\n",
    "throughput = pd.read_csv(f\"{analysis_folder}/throughput_comparison.csv\")\n",
    "print(\"\\n=== Throughput Comparison ===\")\n",
    "display(throughput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best performers\n",
    "best_performers = pd.read_csv(f\"{analysis_folder}/best_performers.csv\")\n",
    "print(\"\\n=== Best Performers ===\")\n",
    "display(best_performers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quality Metrics (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load quality metrics if available\n",
    "quality_file = Path(f\"{analysis_folder}/quality_metrics.csv\")\n",
    "if quality_file.exists():\n",
    "    quality_metrics = pd.read_csv(quality_file)\n",
    "    print(\"\\n=== Quality Metrics ===\")\n",
    "    display(quality_metrics)\n",
    "else:\n",
    "    print(\"No quality metrics available (quality evaluation was skipped)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations\n",
    "\n",
    "Display all generated plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all PNG files\n",
    "plot_files = sorted(Path(analysis_folder).glob(\"*.png\"))\n",
    "\n",
    "for plot_file in plot_files:\n",
    "    print(f\"\\n### {plot_file.stem.replace('_', ' ').title()}\")\n",
    "    display(Image(filename=str(plot_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Report\n",
    "\n",
    "Full text report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display performance report\n",
    "report_file = Path(f\"{analysis_folder}/performance_report.txt\")\n",
    "if report_file.exists():\n",
    "    with open(report_file, 'r') as f:\n",
    "        print(f.read())\n",
    "else:\n",
    "    print(\"No performance report found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Custom Analysis\n",
    "\n",
    "Add your own analysis here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plot tokens/s vs memory\n",
    "if 'tokens_per_s_mean' in quant_summary.columns and 'peak_vram_GB_mean' in quant_summary.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(quant_summary['peak_vram_GB_mean'], quant_summary['tokens_per_s_mean'])\n",
    "    \n",
    "    for idx, row in quant_summary.iterrows():\n",
    "        plt.annotate(row['quantization'], \n",
    "                    (row['peak_vram_GB_mean'], row['tokens_per_s_mean']),\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.xlabel('Peak VRAM (GB)')\n",
    "    plt.ylabel('Tokens per Second')\n",
    "    plt.title('Performance vs Memory Tradeoff')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
